<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Southern</title>
    <link>https://southern-wood.github.io/</link>
    <description>Recent content on Southern</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 08:42:54 +0800</lastBuildDate>
    <atom:link href="https://southern-wood.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Invariant Rule Baseline 复现</title>
      <link>https://southern-wood.github.io/post/nids/</link>
      <pubDate>Tue, 30 Apr 2024 08:42:54 +0800</pubDate>
      <guid>https://southern-wood.github.io/post/nids/</guid>
      <description>项目背景 这是清华大学团队的一个项目，和工业互联网的网络异常检测有关。我和另一位网络工程专业的大二本科生一同参与了这个项目，通过线上会议的形式和清华老师进行交流。&#xA;我们的主要任务是复现相关论文的检测方法，作为论文的 baseline 之一。&#xA;具体复现的论文是 NDSS 2019 的一篇论文：《A Systematic Framework to Generate Invariants for Anomaly Detection in Industrial Control Systems》&#xA;这篇论文提出了一种基于数据驱动的方法，用于生成工业控制系统的不变量，以检测异常的网络状态。论文已经有官方实现：github.com/cfeng783/NDSS19_InvariantRuleAD&#xA;我们的任务是复现这个方法，并在指定格式的数据集和指定的实验设置下进行测试（和清华大学团队已经实现过的 baseline 保持一致）。&#xA;主要问题 代码运行效率低下 在我们着手开始复现这个项目之前，团队应该已经有人简单地在数据集上尝试了官方实现，但是因为效率问题没有得到较好的结果，所以这个工作落到了我们手里。&#xA;老师在实验开始之前就提醒我们，官方实现在我们指定的数据集和实验设置上效率不佳，挖掘规则所花费的时间太久。我们需要尝试提高代码的运行效率，比如查找 CFP-growth++ 算法的高效实现方式，或者分布式算法；如果没有，考虑调整参数，或者限制搜索树高度和搜索空间的大小等。&#xA;不过，这最后被发现是一个简单的数据类型问题，而不是算法本身的效率问题，这一点之后会提到。&#xA;代码逻辑不适配所需的实验设置 官方实现的逻辑如下：&#xA;输入单个训练集和单个测试集 按照训练集的数据特征进行一定的变换，并将这些变换同步到测试集上 在训练集上挖掘规则集 在测试集上评估规则的有效性 我们的实验要求划分十个训练集和大量的不同变化后的测试集，每个训练集挖掘规则后，需要在所有的测试集上分别评估规则的有效性。&#xA;如果按照原逻辑，我们需要循环运行成百上千次代码，对单个训练集进行大量的、重复的规则挖掘。在单次规则挖掘需要花费数小时的情况下，这样的效率是难以忍受的。&#xA;详细情况与解决方案 运行效率：数据类型问题 这是整个复现过程中最关键的问题，它解决起来并不麻烦，但是我们花费了很长时间才发现问题的存在。&#xA;我们在本地使用 csv 格式的 SWaT 和 WADI 数据集，可以复现论文结果。在 WADI 数据集上规则挖掘效率大约在三到五小时一个训练集，这个时间是可以接受的，并没有发现清华老师提到的效率问题；但是当我们使用服务器上的清华团队提供的数据集时，效率问题就出现了。&#xA;清华团队提供的同样是 SWaT 和 WADI 数据集，是用 numpy 的 npy 格式存储的。按照老师的说明，只是对原数据集进行了一些简单的切片，并没有更改数据原始值。但是我们发现，使用这些数据集，规则挖掘的效率会大幅下降，一个训练集的挖掘会长达十几个小时——事实上时间只会更长，因为我们并没有等待规则挖掘结束，而是手动终止了实验。&#xA;这完全不符合我们的预期，因为无论是 csv 还是 npy 格式，都是读入之后转为 pandas 的 DataFrame 进行处理，不应该产生效率上的差别。</description>
    </item>
  </channel>
</rss>
